Development;The process of designing, creating, testing, and implementing software, applications, or systems. It involves writing code, building databases, and creating algorithms to bring a concept or idea to fruition.
Test;The process of evaluating a system or its components to find whether they satisfy the specified requirements. In software development, testing involves running the software or application to identify bugs, errors, or issues, ensuring it functions as intended.
Staging;An environment that mimics the production environment where developers and testers can validate changes before they are deployed to the live (production) system. Staging environments help ensure that new features or updates work correctly and do not disrupt the live system.
Production;Referred to as the production environment, is the live or operational system where end-users access and use the software or application. It is the environment where the final version of the product is deployed and made available to users.
Quality assurance (QA);A systematic process that ensures the quality and reliability of a product or service. In software development, QA involves activities such as process auditing, defect prevention, and process improvement. QA testers are responsible for verifying that the software meets specified requirements and standards before it is released to users.
Provisioning;refers to the process of setting up and configuring IT resources, such as servers, software applications, or user accounts, to ensure they are ready for use. In the context of user accounts, it involves creating and configuring accounts with the necessary permissions, access rights, and resources for individuals within an organization.
Deprovisioning;is the process of revoking access, removing permissions, and disabling user accounts or IT resources that are no longer needed. It ensures that accounts and resources are properly deactivated or deleted when employees leave an organization or when certain resources are no longer required, enhancing security and resource management.
Integrity measurement;the process of verifying and ensuring the trustworthiness and authenticity of data, software, or systems. It involves techniques and mechanisms to detect and prevent unauthorized modifications, tampering, or corruption of data and software components. Integrity measurement methods often use cryptographic hashes or checksums to create unique fingerprints of files or data, allowing systems to verify if they have been altered or compromised. Integrity measurement is crucial for ensuring the reliability and security of computer systems and the data they handle.
Normalization;the process of organizing and structuring data in a database to reduce redundancy and improve data integrity. It ensures that data is efficiently stored without unnecessary duplication, reducing the risk of inconsistencies and vulnerabilities in the database.
Stored procedures;precompiled and stored sets of SQL statements that can be executed by applications or users. They enhance security by allowing controlled access to the database, reducing the risk of SQL injection attacks and providing a layer of abstraction between the user and the database.
Obfuscation/camouflage;techniques used to obscure code or data, making it difficult for unauthorized users to understand or reverse-engineer the software. Obfuscation modifies the code's appearance without changing its behavior, while camouflage involves disguising sensitive information.
Code reuse/dead code;Code reuse involves utilizing existing, tested, and secure code components in new applications to save time and ensure reliability. Dead code refers to code that is no longer used or executed in the program. Removing dead code reduces the attack surface and simplifies the software, making it easier to secure.
Server-side vs. client-side execution and validation;Server-side execution and validation involve performing operations or checks on the server, away from the user's control. This ensures that critical operations are not manipulated by users. Client-side execution and validation occur on the user's device, which can be less secure as it is more susceptible to tampering or interception.
Memory management;handling computer memory to allocate and deallocate memory resources efficiently. Proper memory management prevents buffer overflows and other memory-related vulnerabilities, which attackers often exploit to compromise software.
Use of third-party libraries and software development kits (SDKs);Incorporating third-party libraries and SDKs allows developers to leverage existing, tested code for specific functionalities. However, it's crucial to ensure these libraries are from reputable sources and are regularly updated to address security vulnerabilities and avoid exposing the application to risks.
Data exposure;occurs when sensitive information is unintentionally made accessible to unauthorized users. Secure coding practices aim to prevent data exposure by implementing appropriate access controls, encryption, and ensuring that sensitive data is never exposed through error messages or logs.
Open Web Application Security Project (OWASP);a nonprofit organization that focuses on improving the security of software. OWASP provides freely available resources, tools, and guidelines to help organizations and developers design, develop, and maintain secure web applications and software. The organization is well-known for its OWASP Top Ten project, which lists the top security risks faced by web applications, helping developers prioritize their efforts to secure their applications effectively. OWASP also conducts research and promotes best practices in web application security.
Compiler;a specialized software tool that translates high-level programming code written in languages like C, C++, or Java into machine-readable code or assembly language. It performs various tasks such as lexical analysis, syntax analysis, optimization, and code generation. The output of a compiler is typically a binary file or an executable that can be run on a specific computer architecture.
Binary;a file that contains compiled, machine-readable code. It consists of a sequence of binary digits (0s and 1s), representing instructions that a computer's central processing unit (CPU) can execute directly. Binaries are the result of compiling high-level source code and are specific to the target platform or architecture. When you run a software application, you are essentially executing the instructions stored in its binary form.
Automated courses of action;predefined sequences of automated steps or responses taken by a system or software in response to specific events or conditions. These actions are programmed in advance to handle routine tasks, enhance efficiency, and respond quickly to security threats or other operational needs.
Continuous monitoring;the process of constantly observing and evaluating an IT system's security, performance, and compliance status in real-time or near-real-time. It involves automated tools and scripts that continuously collect and analyze data, allowing organizations to promptly detect and respond to security incidents or vulnerabilities.
Continuous validation;the ongoing verification of software, configurations, or systems to ensure they adhere to predefined standards, security policies, or quality benchmarks. Automation scripts are commonly used to validate code changes, configurations, or system behaviors, ensuring consistency and compliance over time.
Continuous integration;a software development practice where code changes from multiple contributors are automatically integrated into a shared repository multiple times a day. Automated scripts perform tasks such as code compilation, testing, and code quality checks, ensuring that new code changes do not disrupt existing functionality.
Continuous delivery;an extension of continuous integration where code changes are automatically tested and prepared for production deployment. Automated scripts handle tasks such as additional testing, packaging, and versioning, ensuring that software updates are consistently ready for deployment at any time.
Continuous deployment;the practice of automatically deploying code changes to production servers after passing automated tests and other validation processes. This approach enables rapid and frequent releases, as validated code changes are automatically deployed to the production environment without manual intervention.
Elasticity;the ability of a system to automatically scale its resources up or down based on demand. It allows applications or services to dynamically allocate additional resources (such as computing power, storage, or bandwidth) during periods of high demand and release those resources during low-demand periods. Elasticity ensures efficient resource utilization and optimal performance in varying workload situations.
Scalability;the ability of a system, network, or application to handle a growing amount of work, or its potential to be enlarged to accommodate that growth. A scalable system can efficiently handle increased load without compromising performance, allowing it to grow seamlessly as demand increases. Scalability can be achieved through various techniques, such as load balancing, distributed computing, and efficient resource allocation.
Version control;a system that records changes to files or sets of files over time. It allows multiple users or developers to collaborate on a project by tracking modifications, maintaining a historical record of changes, and enabling version comparisons. Version control systems help manage codebase changes, track development progress, and facilitate collaboration, ensuring that software development projects are organized, efficient, and error-free.
